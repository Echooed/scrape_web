{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for parsing HTML and XML documents. Here is the general syntax for using Beautiful Soup:\n",
    "\n",
    "1. Importing the library:\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "2. Creating a BeautifulSoup object:\n",
    "```\n",
    "soup = BeautifulSoup(html_string, 'html.parser')\n",
    "```\n",
    "* `html_string` is the string containing the HTML code.\n",
    "* `'html.parser'` is the parser used to parse the HTML code. You can also use `'lxml'` or `'xml'` parsers.\n",
    "\n",
    "3. Finding elements:\n",
    "```\n",
    "soup.find('tag_name')  # finds the first occurrence of the tag\n",
    "soup.find_all('tag_name')  # finds all occurrences of the tag\n",
    "soup.find('tag_name', {'attribute_name': 'attribute_value'})  # finds the first occurrence of the tag with the specified attribute\n",
    "soup.find_all('tag_name', {'attribute_name': 'attribute_value'})  # finds all occurrences of the tag with the specified attribute\n",
    "```\n",
    "* `tag_name` is the name of the HTML tag you want to find.\n",
    "* `attribute_name` and `attribute_value` are the name and value of the attribute you want to filter by.\n",
    "\n",
    "4. Navigating the tree:\n",
    "```\n",
    "soup.parent  # returns the parent element\n",
    "soup.children  # returns a list of child elements\n",
    "soup.next_sibling  # returns the next sibling element\n",
    "soup.previous_sibling  # returns the previous sibling element\n",
    "```\n",
    "5. Modifying the tree:\n",
    "```\n",
    "soup.tag_name.string  # returns the text content of the tag\n",
    "soup.tag_name.text  # returns the text content of the tag, including child elements\n",
    "soup.tag_name.append(new_tag)  # adds a new tag to the end of the tag\n",
    "soup.tag_name.insert(0, new_tag)  # inserts a new tag at the beginning of the tag\n",
    "soup.tag_name.replace_with(new_tag)  # replaces the tag with a new tag\n",
    "```\n",
    "* `new_tag` is the new tag you want to add or replace.\n",
    "\n",
    "6. Extracting data:\n",
    "```\n",
    "soup.get_text()  # returns the text content of the entire document\n",
    "soup.find('tag_name').get_text()  # returns the text content of the specified tag\n",
    "soup.find('tag_name').attrs  # returns a dictionary of the tag's attributes\n",
    "```\n",
    "These are the basic syntax and methods for using Beautiful Soup. You can find more information and examples in the official Beautiful Soup documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('home.html', 'r') as html_file:\n",
    "    content =html_file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    course_cards =soup.find_all('div', class_ = 'card-body')\n",
    "    for course in course_cards:\n",
    "        course_price = course.a.text.split()[-1]\n",
    "        print(f'The course {course.h5.text} cost ${course_price[:-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practicing, I will scrape data from a job listing website. \n",
    "The details of the extract will \n",
    "\n",
    "- Company name\n",
    "- Job title\n",
    "- Posting date \n",
    "- skills \n",
    "\n",
    "This is just a simple extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get('https://www.timesjobs.com/candidate/job-search.html?from=submit&luceneResultSize=25&txtKeywords=data&postWeek=60&searchType=personalizedSearch&actualTxtKeywords=data&searchBy=0&rdoOperator=OR&pDate=I&sequence=10&startPage=1')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_text =response.text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    job_boxes = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "\n",
    "    job_data = []\n",
    "\n",
    "    for job_box in job_boxes:\n",
    "        company_name = job_box.find('h3', class_='joblist-comp-name').text.strip()\n",
    "        job_title = job_box.find('a').text.strip()\n",
    "        posting_date = job_box.find('span', class_='sim-posted').text.strip()\n",
    "\n",
    "        # Locate the skills container and pass it to the function\n",
    "        skills_container = job_box.find_all('div', class_='more-skills-sections')\n",
    "        \n",
    "\n",
    "        # function to extract skills from with the span tags using regex\n",
    "        def extract_skills(skills_container):\n",
    "            if skills_container:\n",
    "                skills = re.findall(r'<span[^>]*>(.*?)</span>', str(skills_container), re.DOTALL)\n",
    "                skills_list = [skill.strip() for skill in skills if skill.strip()]\n",
    "                return skills_list\n",
    "            return []\n",
    "\n",
    "        skills = extract_skills(skills_container)\n",
    "\n",
    "        job_data.append(\n",
    "            {\n",
    "                'company_name' : {company_name},\n",
    "                'job_title' : {job_title},\n",
    "                'posting_duration': {posting_date},\n",
    "                'skills' : {'|'.join(skills)}\n",
    "            }\n",
    "                     )       \n",
    "        df = pd.DataFrame(job_data)\n",
    "        df.to_csv('job_data.csv', index=False)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get('https://www.timesjobs.com/candidate/job-search.html?from=submit&luceneResultSize=25&txtKeywords=data&postWeek=60&searchType=personalizedSearch&actualTxtKeywords=data&searchBy=0&rdoOperator=OR&pDate=I&sequence=10&startPage=1')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_text =response.text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    job_boxes = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "\n",
    "    for job_box in job_boxes:\n",
    "        company_name = job_box.find('h3', class_='joblist-comp-name').text.strip()\n",
    "        job_title = job_box.find('a').text.strip()\n",
    "        posting_date = job_box.find('span', class_='sim-posted').text.strip()\n",
    "\n",
    "        # Locate the skills container and pass it to the function\n",
    "        skills_container = job_box.find_all('div', class_='more-skills-sections')\n",
    "       \n",
    "        \n",
    "\n",
    "        # function to extract skills from with the span tags using regex\n",
    "        def extract_skills(skills_container):\n",
    "            if skills_container:\n",
    "                skills = re.findall(r'<span[^>]*>(.*?)</span>', str(skills_container), re.DOTALL)\n",
    "                skills_list = [skill.strip() for skill in skills if skill.strip()]\n",
    "                return skills_list\n",
    "            return []\n",
    "\n",
    "        skills = extract_skills(skills_container)\n",
    "\n",
    "        print(f\"Company: {company_name}\")\n",
    "        print(f\"Job Title: {job_title}\")\n",
    "        print(f\"Posting Date: {posting_date}\")\n",
    "        print(f\"Skills: {'|'.join(skills)}\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_skills(skills_container):\n",
    "    # Extract visible and hidden skill elements\n",
    "    skill_elements = skills_container.find_all('span')\n",
    "    # Clean and collect text of each skill\n",
    "    skills = [skill.text.strip() for skill in skill_elements if skill.text.strip()]\n",
    "    return skills\n",
    "\n",
    "response = requests.get('https://www.timesjobs.com/candidate/job-search.html?from=submit&luceneResultSize=25&txtKeywords=data&postWeek=60&searchType=personalizedSearch&actualTxtKeywords=data&searchBy=0&rdoOperator=OR&pDate=I&sequence=10&startPage=1')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_text = response.text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    job_boxes = soup.find_all('ul', class_='more-skills-sections')\n",
    "    \n",
    "    for job_box in job_boxes:\n",
    "        company_name = job_box.find('h3', class_='joblist-comp-name').text.strip()\n",
    "        job_title = job_box.find('a').text.strip()\n",
    "        posting_date = job_box.find('span', class_='sim-posted').text.strip()\n",
    "\n",
    "        # Locate the skills container and pass it to the function\n",
    "        skills_container = job_box.find('div', class_='more-skills-sections')\n",
    "        skills = extract_skills(skills_container)\n",
    "\n",
    "        print(f\"Company: {company_name}\")\n",
    "        print(f\"Job Title: {job_title}\")\n",
    "        print(f\"Posting Date: {posting_date}\")\n",
    "        print(f\"Skills: {', '.join(skills)}\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
