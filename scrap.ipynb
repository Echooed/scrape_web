{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for parsing HTML and XML documents. Here is the general syntax for using Beautiful Soup:\n",
    "\n",
    "1. Importing the library:\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "2. Creating a BeautifulSoup object:\n",
    "```\n",
    "soup = BeautifulSoup(html_string, 'html.parser')\n",
    "```\n",
    "* `html_string` is the string containing the HTML code.\n",
    "* `'html.parser'` is the parser used to parse the HTML code. You can also use `'lxml'` or `'xml'` parsers.\n",
    "\n",
    "3. Finding elements:\n",
    "```\n",
    "soup.find('tag_name')  # finds the first occurrence of the tag\n",
    "soup.find_all('tag_name')  # finds all occurrences of the tag\n",
    "soup.find('tag_name', {'attribute_name': 'attribute_value'})  # finds the first occurrence of the tag with the specified attribute\n",
    "soup.find_all('tag_name', {'attribute_name': 'attribute_value'})  # finds all occurrences of the tag with the specified attribute\n",
    "```\n",
    "* `tag_name` is the name of the HTML tag you want to find.\n",
    "* `attribute_name` and `attribute_value` are the name and value of the attribute you want to filter by.\n",
    "\n",
    "4. Navigating the tree:\n",
    "```\n",
    "soup.parent  # returns the parent element\n",
    "soup.children  # returns a list of child elements\n",
    "soup.next_sibling  # returns the next sibling element\n",
    "soup.previous_sibling  # returns the previous sibling element\n",
    "```\n",
    "5. Modifying the tree:\n",
    "```\n",
    "soup.tag_name.string  # returns the text content of the tag\n",
    "soup.tag_name.text  # returns the text content of the tag, including child elements\n",
    "soup.tag_name.append(new_tag)  # adds a new tag to the end of the tag\n",
    "soup.tag_name.insert(0, new_tag)  # inserts a new tag at the beginning of the tag\n",
    "soup.tag_name.replace_with(new_tag)  # replaces the tag with a new tag\n",
    "```\n",
    "* `new_tag` is the new tag you want to add or replace.\n",
    "\n",
    "6. Extracting data:\n",
    "```\n",
    "soup.get_text()  # returns the text content of the entire document\n",
    "soup.find('tag_name').get_text()  # returns the text content of the specified tag\n",
    "soup.find('tag_name').attrs  # returns a dictionary of the tag's attributes\n",
    "```\n",
    "These are the basic syntax and methods for using Beautiful Soup. You can find more information and examples in the official Beautiful Soup documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The course Python for beginners cost $20\n",
      "The course Python Web Development cost $50\n",
      "The course Python Machine Learning cost $100\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('home.html', 'r') as html_file:\n",
    "    content =html_file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    course_cards =soup.find_all('div', class_ = 'card-body')\n",
    "    for course in course_cards:\n",
    "        course_price = course.a.text.split()[-1]\n",
    "        print(f'The course {course.h5.text} cost ${course_price[:-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practicing, I will scrape data from a job listing website. \n",
    "The details of the extract will \n",
    "\n",
    "- Company name\n",
    "- Job title\n",
    "- Posting date \n",
    "- skills \n",
    "- more details which shows the url to find the job.\n",
    "\n",
    "\n",
    "This is just a simple extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract skill required from job post\n",
    "def extract_skills(skills_container):\n",
    "    if skills_container:\n",
    "        skills = re.findall(r'<span[^>]*>(.*?)</span>', str(skills_container), re.DOTALL)\n",
    "        skills_list = [\n",
    "            skill.strip().replace(' / ', '/').replace('**   ', '').replace('  **', '').replace('amp', '')\n",
    "            for skill in skills if skill.strip()\n",
    "        ]\n",
    "        return skills_list\n",
    "    return []\n",
    "\n",
    "\n",
    "# function to extract data from website html\n",
    "def find_jobs():\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            'https://www.timesjobs.com/candidate/job-search.html?searchType=Home_Search&from=submit&asKey=OFF&txtKeywords=&cboPresFuncArea=&cboWorkExp1=0&clusterName=CLUSTER_EXP'\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            html_text = response.text\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "            job_boxes = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "\n",
    "            for job_box in job_boxes:\n",
    "                job_title = job_box.find('a').text.strip()\n",
    "                posting_date = job_box.find('span', class_='sim-posted').text.strip()\n",
    "                company_name = job_box.find('h3', class_='joblist-comp-name').text.strip()\n",
    "                more_details = job_box.a['href']\n",
    "                    \n",
    "                # Extract skills\n",
    "                skills_container = job_box.find('div', class_='more-skills-sections')\n",
    "                skills = extract_skills(skills_container)\n",
    "    \n",
    "                print(f\"Company: {company_name}\")\n",
    "                print(f\"Job Title: {job_title}\")\n",
    "                print(f\"Posting Date: {posting_date}\")            \n",
    "                print(f\"Skills: {'|'.join(skills)}\")\n",
    "                print(f\"More Details: {more_details}\\n\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"failed to connect\")\n",
    "                  \n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection error. Retrying in 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error: {e}. Retrying in 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Run find_jobs repeatedly with time intervals\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        find_jobs()\n",
    "        time_wait = 10\n",
    "        print(f'Waiting {time_wait} seconds...')\n",
    "        time.sleep(time_wait)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
